require(glmnet)
## PART 1: Load data
library("readxl")
#install.packages("glmnet", dependencies = TRUE)
library(Metrics)
library(ggplot2)
library(MASS)
require(glmnet)
data <- read_excel("tecator.xlsx")
head(data)
plot(data$Moisture, data$Protein, xlab = "Moisture", ylab = "Protein")
## PART #:
# Split into train & test set
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
## Create models
M1 <- lm(Moisture ~ Protein, data = train )
M2 <- lm(Moisture ~ Protein + I(Protein^2), data = train )
M3 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3), data = train )
M4 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4), data = train )
M5 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5), data = train )
M6 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5) + I(Protein^6), data = train )
# Fit M1
M1.test_preds <- predict(M1, test, type = "response")
M1.train_preds <- predict(M1, train, type = "response")
M1.test_mse <- mse(M1.test_preds, test$Moisture)
M1.train_mse <- mse(M1.train_preds, train$Moisture)
# Fit M2
M2.test_preds <- predict(M2, test, type = "response")
M2.train_preds <- predict(M2, train, type = "response")
M2.test_mse <- mse(M2.test_preds, test$Moisture)
M2.train_mse <- mse(M2.train_preds, train$Moisture)
# Fit M3
M3.test_preds <- predict(M3, test, type = "response")
M3.train_preds <- predict(M3, train, type = "response")
M3.test_mse <- mse(M3.test_preds, test$Moisture)
M3.train_mse <- mse(M3.train_preds, train$Moisture)
# Fit M4
M4.test_preds <- predict(M4, test, type = "response")
M4.train_preds <- predict(M4, train, type = "response")
M4.test_mse <- mse(M4.test_preds, test$Moisture)
M4.train_mse <- mse(M4.train_preds, train$Moisture)
# Fit M5
M5.test_preds <- predict(M5, test, type = "response")
M5.train_preds <- predict(M5, train, type = "response")
M5.test_mse <- mse(M5.test_preds, test$Moisture)
M5.train_mse <- mse(M5.train_preds, train$Moisture)
# Fit M6
M6.test_preds <- predict(M6, test, type = "response")
M6.train_preds <- predict(M6, train, type = "response")
M6.test_mse <- mse(M6.test_preds, test$Moisture)
M6.train_mse <- mse(M6.train_preds, train$Moisture)
MSE.test<- c(M1.test_mse,M2.test_mse, M3.test_mse, M4.test_mse, M5.test_mse, M6.test_mse)
MSE.train<- c(M1.train_mse,M2.train_mse, M3.train_mse, M4.train_mse, M5.train_mse, M6.train_mse)
x <- seq(1, 6, by=1)
MSE_res <-
data.frame(
var0 = MSE.test,
var1 = MSE.train,
x = x
)
ggplot(MSE_res, aes(x)) +
geom_line(aes(y = var0, colour = "Test")) +
geom_line(aes(y = var1, colour = "Train")) +
ylab('MSE')+xlab('I')
## Create models
M1 <- lm(Moisture ~ Protein, data = train )
M2 <- lm(Moisture ~ Protein + I(Protein^2), data = train )
M3 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3), data = train )
M4 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4), data = train )
M5 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5), data = train )
M6 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5) + I(Protein^6), data = train )
# Fit M1
M1.test_preds <- predict(M1, test, type = "response")
mean((test$Moisture - M1.test_preds)^2)
mse(M1.test_preds, test$Moisture)
mean((M1.test_preds - test$Moisture)^2)
## Create models
M1 <- lm(Moisture ~ Protein, data = train )
M2 <- lm(Moisture ~ Protein + I(Protein^2), data = train )
M3 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3), data = train )
M4 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4), data = train )
M5 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5), data = train )
M6 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5) + I(Protein^6), data = train )
# Predict M1
M1.test_preds <- predict(M1, test, type = "response")
M1.train_preds <- predict(M1, train, type = "response")
M1.test_mse <- mean((M1.test_preds - test$Moisture)^2)
M1.train_mse <- mean((M1.train_preds - train$Moisture)^2)
# Predict M2
M2.test_preds <- predict(M2, test, type = "response")
M2.train_preds <- predict(M2, train, type = "response")
M2.test_mse <- mean((M2.test_preds - test$Moisture)^2)
M2.train_mse <- mean((M2.train_preds - train$Moisture)^2)
# Predict M3
M3.test_preds <- predict(M3, test, type = "response")
M3.train_preds <- predict(M3, train, type = "response")
M3.test_mse <- mean((M3.test_preds - test$Moisture)^2)
M3.train_mse <- mean((M3.train_preds - train$Moisture)^2)
# Predict M4
M4.test_preds <- predict(M4, test, type = "response")
M4.train_preds <- predict(M4, train, type = "response")
M4.test_mse <- mean((M4.test_preds - test$Moisture)^2)
M4.train_mse <- mean((M4.train_preds - train$Moisture)^2)
# Predict M5
M5.test_preds <- predict(M5, test, type = "response")
M5.train_preds <- predict(M5, train, type = "response")
M5.test_mse <- mean((M5.test_preds - test$Moisture)^2)
M5.train_mse <- mean((M5.train_preds - train$Moisture)^2)
# Predict M6
M6.test_preds <- predict(M6, test, type = "response")
M6.train_preds <- predict(M6, train, type = "response")
M6.test_mse <- mean((M6.test_preds - test$Moisture)^2)
M6.train_mse <- mean((M6.train_preds - train$Moisture)^2)
MSE.test<- c(M1.test_mse,M2.test_mse, M3.test_mse, M4.test_mse, M5.test_mse, M6.test_mse)
MSE.train<- c(M1.train_mse,M2.train_mse, M3.train_mse, M4.train_mse, M5.train_mse, M6.train_mse)
x <- seq(1, 6, by=1)
MSE_res <-
data.frame(
var0 = MSE.test,
var1 = MSE.train,
x = x
)
ggplot(MSE_res, aes(x)) +
geom_line(aes(y = var0, colour = "Test")) +
geom_line(aes(y = var1, colour = "Train")) +
ylab('MSE')+xlab('I')
## Part 4
linearModel <- lm(Fat ~ . - Protein - Moisture - Sample, data = train )
aic <- stepAIC(linearModel)
aic$anova$Step
## Part 4
linearModel <- lm(Fat ~ . - Protein - Moisture - Sample, data = data )
aic <- stepAIC(linearModel)
aic$anova$Step
## Part 4
linearModel <- lm(Fat ~ . - Protein - Moisture - Sample, data = data )
aic <- stepAIC(linearModel)
aic$anova
summary(step)
summary(aic)
aic$anova$Step
aic$anova
?lm
View(aic)
View(aic)
summary(aic$anova)
?glmnet
fit.ridge=glmnet(x,data$Fat,alpha=0, family="gaussian")
plot(fit.ridge,xvar="lambda",label=TRUE)
## PART 1: Load data
library("readxl")
#install.packages("glmnet", dependencies = TRUE)
library(Metrics)
library(ggplot2)
library(MASS)
require(glmnet)
data <- read_excel("tecator.xlsx")
head(data)
plot(data$Moisture, data$Protein, xlab = "Moisture", ylab = "Protein")
## PART #:
# Split into train & test set
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
## Create models
M1 <- lm(Moisture ~ Protein, data = train )
M2 <- lm(Moisture ~ Protein + I(Protein^2), data = train )
M3 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3), data = train )
M4 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4), data = train )
M5 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5), data = train )
M6 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5) + I(Protein^6), data = train )
# Predict M1
M1.test_preds <- predict(M1, test, type = "response")
M1.train_preds <- predict(M1, train, type = "response")
M1.test_mse <- mean((M1.test_preds - test$Moisture)^2)
M1.train_mse <- mean((M1.train_preds - train$Moisture)^2)
# Predict M2
M2.test_preds <- predict(M2, test, type = "response")
M2.train_preds <- predict(M2, train, type = "response")
M2.test_mse <- mean((M2.test_preds - test$Moisture)^2)
M2.train_mse <- mean((M2.train_preds - train$Moisture)^2)
# Predict M3
M3.test_preds <- predict(M3, test, type = "response")
M3.train_preds <- predict(M3, train, type = "response")
M3.test_mse <- mean((M3.test_preds - test$Moisture)^2)
M3.train_mse <- mean((M3.train_preds - train$Moisture)^2)
# Predict M4
M4.test_preds <- predict(M4, test, type = "response")
M4.train_preds <- predict(M4, train, type = "response")
M4.test_mse <- mean((M4.test_preds - test$Moisture)^2)
M4.train_mse <- mean((M4.train_preds - train$Moisture)^2)
# Predict M5
M5.test_preds <- predict(M5, test, type = "response")
M5.train_preds <- predict(M5, train, type = "response")
M5.test_mse <- mean((M5.test_preds - test$Moisture)^2)
M5.train_mse <- mean((M5.train_preds - train$Moisture)^2)
# Predict M6
M6.test_preds <- predict(M6, test, type = "response")
M6.train_preds <- predict(M6, train, type = "response")
M6.test_mse <- mean((M6.test_preds - test$Moisture)^2)
M6.train_mse <- mean((M6.train_preds - train$Moisture)^2)
MSE.test<- c(M1.test_mse,M2.test_mse, M3.test_mse, M4.test_mse, M5.test_mse, M6.test_mse)
MSE.train<- c(M1.train_mse,M2.train_mse, M3.train_mse, M4.train_mse, M5.train_mse, M6.train_mse)
x <- seq(1, 6, by=1)
MSE_res <-
data.frame(
var0 = MSE.test,
var1 = MSE.train,
x = x
)
ggplot(MSE_res, aes(x)) +
geom_line(aes(y = var0, colour = "Test")) +
geom_line(aes(y = var1, colour = "Train")) +
ylab('MSE')+xlab('i')
## Part 4
linearModel <- lm(Fat ~ . - Protein - Moisture - Sample, data = data )
aic <- stepAIC(linearModel)
summary(aic$anova)
x=model.matrix(Fat~. - Protein - Moisture - Sample,data=data)
fit.ridge=glmnet(x,data$Fat,alpha=0, family="gaussian")
plot(fit.ridge,xvar="lambda",label=TRUE)
fit.ridge=glmnet(x,data$Fat,alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE)
fit.lasso=glmnet(x,data$Fat,alpha=1)
plot(fit.lasso,xvar="lambda",label=TRUE)
fit.lasso=glmnet(x,data$Fat,alpha=1, family="gaussian")
plot(fit.lasso,xvar="lambda",label=TRUE)
cv.lasso=cv.glmnet(x,data$Fat, alpha=1, family="gaussian")
cv.lasso$lambda.min
cv.lasso=cv.glmnet(x,data$Fat, alpha=1)
cv.lasso$lambda.min
fit.ridge=glmnet(x,data$Fat,alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE)
fit.lasso=glmnet(x,data$Fat,alpha=1)
plot(fit.lasso,xvar="lambda",label=TRUE)
## PART 1: Load data
library("readxl")
#install.packages("glmnet", dependencies = TRUE)
library(Metrics)
library(ggplot2)
library(MASS)
require(glmnet)
data <- read_excel("tecator.xlsx")
head(data)
plot(data$Moisture, data$Protein, xlab = "Moisture", ylab = "Protein")
## PART #:
# Split into train & test set
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
## Create models
M1 <- lm(Moisture ~ Protein, data = train )
M2 <- lm(Moisture ~ Protein + I(Protein^2), data = train )
M3 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3), data = train )
M4 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4), data = train )
M5 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5), data = train )
M6 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5) + I(Protein^6), data = train )
# Predict M1
M1.test_preds <- predict(M1, test, type = "response")
M1.train_preds <- predict(M1, train, type = "response")
M1.test_mse <- mean((M1.test_preds - test$Moisture)^2)
M1.train_mse <- mean((M1.train_preds - train$Moisture)^2)
# Predict M2
M2.test_preds <- predict(M2, test, type = "response")
M2.train_preds <- predict(M2, train, type = "response")
M2.test_mse <- mean((M2.test_preds - test$Moisture)^2)
M2.train_mse <- mean((M2.train_preds - train$Moisture)^2)
# Predict M3
M3.test_preds <- predict(M3, test, type = "response")
M3.train_preds <- predict(M3, train, type = "response")
M3.test_mse <- mean((M3.test_preds - test$Moisture)^2)
M3.train_mse <- mean((M3.train_preds - train$Moisture)^2)
# Predict M4
M4.test_preds <- predict(M4, test, type = "response")
M4.train_preds <- predict(M4, train, type = "response")
M4.test_mse <- mean((M4.test_preds - test$Moisture)^2)
M4.train_mse <- mean((M4.train_preds - train$Moisture)^2)
# Predict M5
M5.test_preds <- predict(M5, test, type = "response")
M5.train_preds <- predict(M5, train, type = "response")
M5.test_mse <- mean((M5.test_preds - test$Moisture)^2)
M5.train_mse <- mean((M5.train_preds - train$Moisture)^2)
# Predict M6
M6.test_preds <- predict(M6, test, type = "response")
M6.train_preds <- predict(M6, train, type = "response")
M6.test_mse <- mean((M6.test_preds - test$Moisture)^2)
M6.train_mse <- mean((M6.train_preds - train$Moisture)^2)
MSE.test<- c(M1.test_mse,M2.test_mse, M3.test_mse, M4.test_mse, M5.test_mse, M6.test_mse)
MSE.train<- c(M1.train_mse,M2.train_mse, M3.train_mse, M4.train_mse, M5.train_mse, M6.train_mse)
x <- seq(1, 6, by=1)
MSE_res <-
data.frame(
var0 = MSE.test,
var1 = MSE.train,
x = x
)
ggplot(MSE_res, aes(x)) +
geom_line(aes(y = var0, colour = "Test")) +
geom_line(aes(y = var1, colour = "Train")) +
ylab('MSE')+xlab('i')
## Part 4
linearModel <- lm(Fat ~ . - Protein - Moisture - Sample, data = data )
aic <- stepAIC(linearModel)
summary(aic$anova)
x=model.matrix(Fat~. - Protein - Moisture - Sample,data=data)
fit.ridge=glmnet(x,data$Fat,alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE)
fit.lasso=glmnet(x,data$Fat,alpha=1)
plot(fit.lasso,xvar="lambda",label=TRUE)
cv.lasso=cv.glmnet(x,data$Fat, alpha=1)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
cv.lasso=cv.glmnet(x,data$Fat, alpha=1)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
coef(cv.lasso, s="lambda.min")
cv.lasso$lambda.min
## cross-validation
lambdas = exp(seq(-4.5, 6.5, 0.1))
cv.lasso=cv.glmnet(x,data$Fat, alpha=1, lambda = lambdas)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
## cross-validation
lambdas = exp(seq(-1, 1, 0.1))
cv.lasso=cv.glmnet(x,data$Fat, alpha=1, lambda = lambdas)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
## cross-validation
lambdas = exp(seq(-10, 10, 0.1))
cv.lasso=cv.glmnet(x,data$Fat, alpha=1, lambda = lambdas)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
## cross-validation
lambdas = exp(seq(-20, 10, 0.1))
cv.lasso=cv.glmnet(x,data$Fat, alpha=1, lambda = lambdas)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
## cross-validation
cv.lasso=cv.glmnet(x,data$Fat, alpha=1)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
## cross-validation
lambdas = seq(-10, 10, 0.1)
cv.lasso=cv.glmnet(x,data$Fat, alpha=1, lambda = lambdas)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
## cross-validation
lambdas = seq(-2, 2, 0.1)
cv.lasso=cv.glmnet(x,data$Fat, alpha=1, lambda = lambdas)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
## cross-validation
lambdas = seq(-2, 2, 0.1)
cv.lasso=cv.glmnet(x,data$Fat, alpha=1, lambda = lambdas)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
## cross-validation
lambdas = seq(-1, 2, 0.1)
cv.lasso=cv.glmnet(x,data$Fat, alpha=1, lambda = lambdas)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
cv.lasso$lambda.min
## cross-validation
cv.lasso=cv.glmnet(x,data$Fat, alpha=1)
cv.lasso$lambda.min
## Part 4
linearModel <- lm(Fat ~ . - Protein - Moisture - Sample, data = data )
aic <- stepAIC(linearModel)
summary(aic$anova)
summary(aic$anova)
aic$anova
## cross-validation
cv.lasso=cv.glmnet(x,data$Fat, alpha=1)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
cv.lasso$lambda.min
# Loading data
library("readxl")
#library("caret")
library("xtable")
library("kknn")
data <- read_excel("spambase.xlsx")
# make spam categorical
data$Spam <- as.factor(data$Spam)
# Split into train & test set
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
# Fit the model on train set and predict on train
logRegModel <- glm(Spam ~ ., data = train, family = binomial)
#predict on train set with 0.5 cutoff
train_probs <- predict(logRegModel, train, type = "response")
train_pred <- ifelse(train_probs > 0.5, "1", "0")
table("Actual" = train$Spam, "Predicted" = train_pred )
mean(train_pred != train$Spam)
#predict on test set with 0.5 cutoff
test_probs <- predict(logRegModel, test, type = "response")
test_pred <- ifelse(test_probs > 0.5, "1", "0")
table("Actual" = test$Spam, "Predicted" = test_pred )
mean(test_pred != test$Spam)
#predict on train set with 0.8 cutoff
train_pred08 <- ifelse(train_probs > 0.8, "1", "0")
table("Actual" = train$Spam, "Predicted" = train_pred08 )
mean(train_pred08 != train$Spam)
#predict on test set with 0.8 cutoff
test_pred08 <- ifelse(test_probs > 0.8, "1", "0")
table("Actual" = test$Spam, "Predicted" = test_pred08 )
mean(test_pred08 != test$Spam)
kknn_classifier = kknn(Spam ~ ., train, train, k=30)
View(kknn_classifier)
View(kknn_classifier)
kknn_classifier[["fitted.values"]]
## Part 4
linearModel <- lm(Fat ~ . - Protein - Moisture - Sample, data = data )
## PART 1: Load data
library("readxl")
#install.packages("glmnet", dependencies = TRUE)
library(Metrics)
library(ggplot2)
library(MASS)
require(glmnet)
data <- read_excel("tecator.xlsx")
head(data)
plot(data$Moisture, data$Protein, xlab = "Moisture", ylab = "Protein")
## PART #:
# Split into train & test set
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
## Create models
M1 <- lm(Moisture ~ Protein, data = train )
M2 <- lm(Moisture ~ Protein + I(Protein^2), data = train )
M3 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3), data = train )
M4 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4), data = train )
M5 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5), data = train )
M6 <- lm(Moisture ~ Protein + I(Protein^2) + I(Protein^3) + I(Protein^4) + I(Protein^5) + I(Protein^6), data = train )
# Predict M1
M1.test_preds <- predict(M1, test, type = "response")
M1.train_preds <- predict(M1, train, type = "response")
M1.test_mse <- mean((M1.test_preds - test$Moisture)^2)
M1.train_mse <- mean((M1.train_preds - train$Moisture)^2)
# Predict M2
M2.test_preds <- predict(M2, test, type = "response")
M2.train_preds <- predict(M2, train, type = "response")
M2.test_mse <- mean((M2.test_preds - test$Moisture)^2)
M2.train_mse <- mean((M2.train_preds - train$Moisture)^2)
# Predict M3
M3.test_preds <- predict(M3, test, type = "response")
M3.train_preds <- predict(M3, train, type = "response")
M3.test_mse <- mean((M3.test_preds - test$Moisture)^2)
M3.train_mse <- mean((M3.train_preds - train$Moisture)^2)
# Predict M4
M4.test_preds <- predict(M4, test, type = "response")
M4.train_preds <- predict(M4, train, type = "response")
M4.test_mse <- mean((M4.test_preds - test$Moisture)^2)
M4.train_mse <- mean((M4.train_preds - train$Moisture)^2)
# Predict M5
M5.test_preds <- predict(M5, test, type = "response")
M5.train_preds <- predict(M5, train, type = "response")
M5.test_mse <- mean((M5.test_preds - test$Moisture)^2)
M5.train_mse <- mean((M5.train_preds - train$Moisture)^2)
# Predict M6
M6.test_preds <- predict(M6, test, type = "response")
M6.train_preds <- predict(M6, train, type = "response")
M6.test_mse <- mean((M6.test_preds - test$Moisture)^2)
M6.train_mse <- mean((M6.train_preds - train$Moisture)^2)
MSE.test<- c(M1.test_mse,M2.test_mse, M3.test_mse, M4.test_mse, M5.test_mse, M6.test_mse)
MSE.train<- c(M1.train_mse,M2.train_mse, M3.train_mse, M4.train_mse, M5.train_mse, M6.train_mse)
x <- seq(1, 6, by=1)
MSE_res <-
data.frame(
var0 = MSE.test,
var1 = MSE.train,
x = x
)
ggplot(MSE_res, aes(x)) +
geom_line(aes(y = var0, colour = "Test")) +
geom_line(aes(y = var1, colour = "Train")) +
ylab('MSE')+xlab('i')
## Part 4
linearModel <- lm(Fat ~ . - Protein - Moisture - Sample, data = data )
aic <- stepAIC(linearModel)
aic$anova
View(aic)
## cross-validation
cv.lasso=cv.glmnet(x,data$Fat, alpha=1)
cv.lasso$lambda.min
coef(cv.lasso, s="lambda.min")
plot(cv.lasso)
